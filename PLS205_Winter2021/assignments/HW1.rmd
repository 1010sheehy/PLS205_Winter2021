---
title: "HW 1"
output: 
 html_notebook:
    toc: true
    toc_float: true
    number_sections: true
---

Use this R Notebook document to answer the questions and document your work. 
Enter the R code used to answer each question in the corresponding R code chunk. 
Write any textual explanations **outside** of the chunks. 
Attempt to clean up your code as much as possible so that only the necessary lines remain. 

When you are done:

1. Select 'Run All' from the 'Run' dropdown menu.
2. Save (File -> Save)
3. Click 'Preview' to bring up the `HW1.nb.html` file. Check through this to make sure it rendered correctly.
4. Upload the files: `HW1.nb.html` and `HW1.Rmd` to Canvas.

---

# Part 1

Draw a diagram of an experiment that you are currently running, one that you have run in the past, or will run soon, 
or make up an experiment that is relevant to your research. Label the diagram using the terms we discussed in the first class. 
Try to show how you'll lay out your experiment; you don't have to be precise about where every sample will come from,
but try to show the overall structure of how you'll lay out your sampling strategy, how you'll assign treatments, and
what you'll measure. You can include a couple sentences below to help explain if it's helpful. The goal of this is to get you thinking
about the issues in experimental design, and to help me understand the range of questions students are studying.

To upload your diagram, save it as a file (jpeg, png, etc, but not pdf) in the directory of this .Rmd file, and include it's file name below:


> ![Experimental design for effect of audience familiarity on aggressive interactions in the Amazon molly, _P. formosa._](P.formosa_Familiarity_ExperimentalDesignSetup.jpg)


**Note:** You can use PowerPoint or a drawing tool to create the diagram on your computer, or you can draw on paper, take a picture with your phone, and then download that file here.
    

---

# Part 2

When calculating $s^2$ to estimate the population variance ($\sigma^2$), we divide by $n-1$ instead of $n$. 
I mentioned in class that this was because once we know the **mean of a sample** ($\bar{y}$) and the first $n-1$ values,
we can calculate the $n$th value, so this last number is redundant; it doesn't contain any new information for us to use.

> Note: Put your cursor between each $..$ and hover your mouse over the text to see it rendered into pretty mathematics 

## Prove this by calculating the $n$th value for the data below:
Data: Plant heights (cm)
sample: 10, 18, 23, 9, 15, 12, 15, n
mean: 15.5 cm

**Solving for n**
```{r}
# solving the algebraic problem, (10+18+23+9+15+12+15+n)/8 = 15.5

15.5*8-(10+18+23+9+15+12+15)

```

> n = 22 cm

## Estimate the Standard Deviation of the full population

```{r include=FALSE}
hw1 <- c(10, 18, 23, 9, 15, 12, 15, 22)
```

**Calculating the Standard Deviation**
```{r include = TRUE}
hw1_mean <- sum(hw1)/8                   # recalculating the mean
hw1_mean

hw1_diff <- hw1 - hw1_mean               # calculating the difference
hw1_diff

hw1_sumdifsqr <- sum((hw1_diff)^2)       # adding up the sqr of the difference
hw1_sumdifsqr

hw1_sd <- sqrt( hw1_sumdifsqr / (8-1) )  # sqrt of the sum of  sqr of the diff
hw1_sd                                   # print the standard deviation


```

> SD = 5.209881 cm

## Estimate the Standard Error of the mean

**Estimating the SE of the mean**
```{r}
hw1_se <- hw1_sd / sqrt(8) # calculating the standard error
hw1_se                     # print the standard error
```

> SE = 1.841971 cm

## Estimate the Coefficient of Variation of this population
The coefficient of variation (or CV) is defined as $\sigma/\mu$, i.e. the standard deviation divided by the population mean.
This is a normalized measure of the amount of variation in a population.

**Estimating the Coefficient of Variation**
```{r}
hw1_cv <- hw1_sd / hw1_mean # calculating the coefficient of variation
hw1_cv                      # print the coeff. of var.
```

> Coefficient of Variation = 0.3361213

a) What are the units of CV?

> There are no units for the coefficient of variation.

b) Say the original data were reported in mm instead of cm. How would the standard deviation of the sample change? How would the CV of the population change?

```{r include = FALSE}
hw1_mm <- hw1 * 10

hw1_mm_mean <- mean(hw1_mm)                  # recalculating the mean

hw1_mm_diff <- hw1_mm - hw1_mm_mean          # calculating the difference

hw1_mm_sumdifsqr <- sum((hw1_mm_diff)^2)     # adding up the square of the diff

hw1_mm_sd <- sqrt( hw1_mm_sumdifsqr / (8-1) )# sqrt of the sum of sqr of the diff
hw1_mm_sd                                    # print the standard deviation

hw1_mm_cv <- hw1_mm_sd / hw1_mm_mean         # calc the coefficient of variation
hw1_mm_cv

```

> By converting cm to mm, we multiply every value in the dataset by 10. This has the same effect on the sd, increasing the sd from 5.209881 to 52.09881

> There are no changes in the CV. This is because the data and the mean are changed in equal proportions, leading to the same value of of sd/mean.

c) Add 20cm to each value in the original sample (including the $n$th value). How would the standard deviation of the sample change? How would the CV of the population change?

```{r include=FALSE}
hw1_plus20 <- hw1 + 20

hw1_plus20_sd <- sd(hw1_plus20)
hw1_plus20_sd

hw1_plus20_cv <- hw1_plus20_sd / mean(hw1_plus20)
hw1_plus20_cv

```

> The SD does not change, as adding 20 to each value only serves to shift the curve on the x axis. It does not change the shape of the curve.

> However, the coefficient of variation does change from 0.3361213 to 0.1467572. This is because adding 20 to the values increases the mean of the data, which is in the denominator of the coefficient of variation formula, thus decreasing the size of the coefficient.


---

# Part 3

The following code runs a simulation of an experiment like we ran in class. 
Each iteration of the for-loop, we sample the pulse of someone sitting and someone standing and record those in two data vectors.
Run the simulation by clicking the green arrow in the code block below.

> Note: While the simulation is random, by putting `set.seed(1)` in the first line, we will always get the same random draws. But if you change the seed to a different number you'll get a different (but equally valid) simulated experiment.

```{r}
set.seed(1) # this makes the results repeatable 
simulation1_sitting = c()
simulation1_standing = c()
for(i in 1:20) {
    # measure a person sitting
    baseline_i_sitting = rnorm(n = 1,mean = 80,sd = 10)  # their true baseline pulse
    person_sitting_i = round(baseline_i_sitting + rnorm(n=1,mean = 0,sd = 1)) # measured pulse - includes some measurement error and is rounded to the nearest 1 bpm
    simulation1_sitting[i] = person_sitting_i
    # measure a person standing
    baseline_i_standing = rnorm(n = 1,mean = 90,sd = 10) # their true baseline pulse
    person_standing_i = round(baseline_i_standing + rnorm(n=1,mean = 0,sd = 1)) # measured pulse
    simulation1_standing[i] = person_standing_i
}
print("Sitting data")
print(simulation1_sitting)
print("Standing data")
print(simulation1_standing)
```

## Answer the following questions about this simulation:
a) How many people were in each treatment?

> 20 people per treatment

b) What is the TRUE effect of the treatment (in this simulation)?

> The true effect is an increase from a mean of 80 bpm to 90 bpm, a difference of 10 bpm.

c) What is the TRUE population variance?

```{r include = FALSE}

baseline_i_cv <- 10 / ((80+90)/2)
baseline_i_cv

```

> The true population variance is 0.1176471.

d) What is the TRUE Standard Error of the Difference for this experiment?
```{r include = FALSE}
baseline_i_sitting_se <- 10 / sqrt(20)
baseline_i_sitting_se

baseline_i_standig_se <- 10 / sqrt(20)
baseline_i_standig_se

```

> The true standard error for sitting is 2.236068.

## Estimate the effect of standing on pulse in this simulated experiment and compare your answer with the TRUE value.

**Estimating the effect of standing**
```{r include = TRUE}
simulation1_standing_effect <- mean(simulation1_standing) - mean(simulation1_sitting)
simulation1_standing_effect
```

> The effect of standing on our simulated experiment is an increase of 12.3 bpm. This is slightly higher than the true value, 10 bpm.

## What is the Error in your estimate? How does this compare to the TRUE Standard Error of the difference?

```{r include = FALSE}

simulation1_se <- 12.3 / sqrt(20)
simulation1_se



```
> The error in my estimate is 2.750364. This is also slightly higher than the true se, 2.236068.

-----

The following code runs a slight modification to the above experiment. 
Each iteration of the for-loop, instead of sampling two different people, we sample a single person and measure 
their pulse both sitting and standing.

> Note: in a real experiment, we'd randomize whether the sitting or standing treatment happend first. 
Here I've skipped this to make the code simpler
> Note2: You don't need to be able to create R code like below in this course, but it's good practice trying to read it and try to figure out how it works.

```{r}
set.seed(1)
simulation2 = c()
for(i in 1:20) {
    baseline_i_pulse = rnorm(n=1,mean=80,sd = 10)   # baseline pulse of this person
    standing_i_effect = rnorm(n=1,mean=10,sd = 2)   # effect of standing on the pulse for this person
    sitting_pulse_i = round(baseline_i_pulse + rnorm(1,0,1))  # measured pulse when sitting
    standing_pulse_i = round(baseline_i_pulse + standing_i_effect + rnorm(1,0,1)) # measured pulse when standing
    treatment_effect_i = standing_pulse_i - sitting_pulse_i  # observed difference between sitting and standing for this person
    simulation2 = rbind(simulation2,data.frame(Person = i, Sitting = sitting_pulse_i, Standing = standing_pulse_i, Effect = treatment_effect_i))  # collect the results
}
simulation2
```
## Estimate the effect of standing on pulse in this second simulated experiment and compare your answer with the TRUE value.

**Estimating the effect of standing**
```{r}
simulation2_effect <- mean(simulation2$Effect)
simulation2_effect
```

> The effect of standing in simulation2 is 9.6 bpm, slightly lower than the true effect, 10 bpm.

## Speculate on why the effect estimate in the second experiment was more accurate
Cover these areas:

a) Was more data collected in the second experiment?

> Instead of two treatment groups of ten, we sampled the same 10 people twice. This is an example of pseudo replication; we did not collect more data in the second experiment.

b) Was the variance of the sitting pulses smaller? The variance of the standing pulses?

```{r include = FALSE}
simulation2_sit_var <- sd(simulation2$Sitting) / mean(simulation2$Sitting)
simulation2_sit_var

simulation1_sitting_var <- sd(simulation1_sitting) / mean(simulation1_sitting)
simulation1_sitting_var

simulation2_stand_var <- sd(simulation2$Standing) / mean(simulation2$Standing)
simulation2_stand_var

simulation1_stand_var <- sd(simulation1_standing) / mean(simulation1_standing)
simulation1_stand_var
```

> The variance of the sitting pulses in the second experiment (0.09327363) were slightly lower than those in the first experiment (0.09344243). The standing pulse for the second experiment (0.08421494) were also lower than those in the first (0.1032855).

c) Was the second experiment more controlled?

> Yes, the second experiment was more controlled as we were comparing sitting and standing within an individual.

d) Are the two experiments designed to measure the same thing?

> No, the two experiments do not measure the same thing. Experiment one compares the effect of standing on a population, while the second examines the effect on an individual.

-----

In general, we only run an experiment once, or maybe a couple times. But on the computer we can run experiments hundreds of times and compare the results. Repeating experiments over and over is the conceptual idea behind "Standard Errors", Confidence intervals and p-values which we use to report what could happen in a replicate experiment. We use statistics to substitute for the effort of actually repeating experiments many times. But on the computer we can just do the replication directly!

The following code repeats the two experiments 100 times each, and collects the answers from each replicate.
Each iteration is a completely new experiment with new experimental units and new measurement errors.

```{r}
set.seed(1)
# repeat experiment-type1 100 times
type_1_results = c()
for(j in 1:100) {  # Each iteration of this for-loop we run both the Type1 and Type2 experiments
    simulation1_sitting = c()
    simulation1_standing = c()
    for(i in 1:20) {
        # measure person sitting
        baseline_i_sitting = rnorm(n = 1,mean = 80,sd = 10)  # true pulse
        person_sitting_i = round(baseline_i_sitting + rnorm(n=1,mean = 0,sd = 1)) # measured_pulse - rounded to the 1 bpm
        simulation1_sitting[i] = person_sitting_i
        # measure person standing
        baseline_i_standing = rnorm(n = 1,mean = 90,sd = 10) # true pulse
        person_standing_i = round(baseline_i_standing + rnorm(n=1,mean = 0,sd = 1)) # measured_pulse
        simulation1_standing[i] = person_standing_i
    }
    type_1_results[j] = mean(simulation1_standing) - mean(simulation1_sitting)
}
# repeat experiment-type2 100 times
type_2_results = c()
for(j in 1:100) {
    simulation2 = c()
    for(i in 1:20) {
        baseline_i_pulse = rnorm(n=1,mean=80,sd = 10)
        standing_i_effect = rnorm(n=1,mean=10,sd = 2)
        sitting_pulse_i = round(baseline_i_pulse + rnorm(1,0,1))
        standing_pulse_i = round(baseline_i_pulse + standing_i_effect + rnorm(1,0,1))
        treatment_effect_i = standing_pulse_i - sitting_pulse_i
        simulation2 = rbind(simulation2,data.frame(Person = i, Sitting = sitting_pulse_i, Standing = standing_pulse_i, Effect = treatment_effect_i))
    }
    type_2_results[j] = mean(simulation2$Effect)
}
boxplot(list(Type1 = type_1_results,Type2 = type_2_results))
# abline(h = 10)
```

## Calculate the standard deviation of the effect estimates for the Type1 and Type2 experiment
Compare these values to the TRUE Standard Error of a difference for the first experiment.
For an extra point, also calculate the TRUE Standard Error for the Type2 experiment.

**Calculating the SD of the effect estimates for Type1 and Type2**
```{r}

type1_sd <- sd(type_1_results)
type1_sd

type2_sd <- sd(type_2_results)
type2_sd
```

**Calculating the TRUE SE for the Type2 Experiment**
```{r}
type2_se <- 10 / sqrt(10)
type2_se
```
> The type1 SD was 2.849405 and the type2 sd was 0.5110644. The true SE for the first experiment was 2.236068. As illustrated by the graph output of the final simulation, the type1 experiment has a much larger variance in the effect size than the type2 experiment. This is because in the type1 experiment, we are comparing two _independent_ groups (sitting and standing). Each group might be above or below the true mean. This means that occasionally, by chance, the sitting group would be below the mean while the standing group is above, leading to a larger difference. This does not happen with the type2 experiment, as the sample of indiviudals are measured repeatedly. So if a sample happens to be below the true mean when sitting, they will also likely be below the true mean while standing.

> Bonus, TRUE standard error for Type2 = 3.162278.


