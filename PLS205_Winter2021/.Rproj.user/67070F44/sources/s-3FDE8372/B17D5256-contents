---
title: "HW 8"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
    toc_float: yes
---

```{r echo=FALSE,message=FALSE,warning=FALSE}
library(lme4)
library(lmerTest)
library(pbkrtest)
library(emmeans)
library(ggplot2)
library(car)
library(multcomp)
```

---


This homework follows from Lab 8. You'll analyze the Split-Plot Oats dataset in several different ways and compare results.

The experiment is modified slightly. Now rather than 4 blocks in one field, the experiment
was done over 4 fields, with each Fertilizer treatment applied once to a single section of each field.

In this study, four different fertilizers were randomized within four MainPlots in each of four fields. 
Four different varieties of oats were randomized within four subplots separately in each of the sixteen main plots. 
The experiment was run as a split-plot because the fertilizers could only be applied to large sections of the field,
while the oat varieties could be randomized to smaller plots within each section.

For the following analyses, we'll assume the the Blocks are actually four separate fields.

- Fields: Four
- Main plots (Fertilizer): Four
- Subplots (Variety): Four (includes one control, V1)

![](Oats_layout.png)

```{r}
oats_data = read.csv('Oats_Field.csv')
str(oats_data)
```

The following code fits the model demonstrated in lab:

```{r}
fixed_block_model = lmer(Yield ~ Fertilizer + Field + (1|MainPlot) + Variety + Fertilizer:Variety + Variety:Field,oats_data)
```

## Question 1. Split Plot RCBD with Random blocks

### 1.1 Describe the experiment in detail
Modify the experimental table from Lab 8 for the case where the four Fields are considered random.

#NOTES
Four fields are NOW random, weren't before
GENERALIZING RESULTS - working with MAIN EFFECTS
Everything nested within field is random
Each variety over the field will be regarded together so var:field becomes EU of variety rather than last thing

**Design**: Split-plot with RCBD on main plots

| Structure | Variable                      | Type        | # levels | Experimental Unit          |
|-----------|-------------------------------|-------------|----------|----------------------------|
| Treatment | Fertilizer                    |   Cat       |   4      |  MainPlot  or Fer:Field    |
|           | Variety                       |   Cat       |   4      |  Var:Field                 |
|           | Fertilizer:Variety            |   Cat       |   16     |  Fe:Var:Field              |
| Design    | Field                         |   Cat       |   4      |                            |
|           | Main Plot                     |   Cat       |   16     |                            |
|           | Sub Plot                      |   Cat       |   64     |                            |
|           | Fertilizer:Field              |   Cat       |   16     |                            |
|           | Variety: Field                |   Cat       |   16     |                            |
|           | Fertilizer:Var:Field          |   Cat       |   64     |                            |
|           | Var:Main Plot                 |   Cat       |   64     |                            |
|           | Fertilizer:Var:Main Plot      |   Cat       |   64     |                            |
| Response  | Yield                         |   Num       |   64     |                            |


### 1.2 Describe how your conclusions from this analysis will differ from those from lab.
Don't analyze the data yet. Just describe how statements about effects you observe will change in this case.

> Now that we're considering Field to be random (and thus all our nested terms to be random), our model and conclusions will be assessing the generalizability of the effects of the treatments. Because we're now looking at random effects, my EU's above reflect the EU's we'll choose that can be used for generalizable models across "new" fields. This shift in EU reduces our df, since we're going from 64 levels down to 16 levels. We expect any statistics that rely on Df to change as a consequence of this.


### 1.3 Fit an appropriate linear model to the data
Modify the following code from lab according to account for the random blocks
```{r}
# use the following name for your model by removing the '#' and filling in the rest
#random_block_model = 
random_block_model <- lmer(Yield ~ Fertilizer + Variety + Fertilizer:Variety +  # treatment terms
                           Field + (1|MainPlot) + 
                             # Design terms. SubPlot not included because aliased with Yield 
                           (1|Variety:Field)
                             # Fertilizer:Field is not included because we included MainPlot and they are aliased
                           # Fertilizer:Variety:Field, Variety:MainPlot, Fertilizer:Variety:MainPlot - not included because aliased with Yield
                          ,data = oats_data
                          )



```

### 1.4 Make ANOVA tables for the `fixed_block_model` and `random_block_model`
List differences between the results. Ignore rows involving `Block`, and focus on `Mean Sq`, `NumDF`, `DenDF`, `F.value`, and `Pr(>F)`
Which results are the same? Which are different?

Fixed Blocks ANOVA:
```{r}
# from lab:
anova(fixed_block_model,ddf='K')
```

Random Blocks ANOVA:
```{r}
anova(random_block_model,ddf='K')

```

> The values for Mean Squares, NumDF, DenDF, F value and P value stayed almost exactly the same for Fertilizer & the Fertilizer:Variety effects between the two models. For Variety, the mean squares, DenDF, and F Value decreased from fixed to random, while the the p-value increased. Field:Variety doesn't exist in the second model since that's now our selected EU. 

### 1.5 In lab, we observed that different comparisons of specific combinations of Fertilizer and Genotype were estimated with different SEs (last part of section "Means comparisons").
Using the same code, inspect the SEs of all pairwise comparisons of combinations of Fertilizer and Genotype for the model with **random blocks**.
There are now three different "classes" of comparisons, each with a different SE. What are these three classes?
```{r}
all_means = emmeans(random_block_model,specs = c('Fertilizer','Variety'),lmer.df = 'K')
all_differences = contrast(all_means,method = 'pairwise')
all_contrasts = as.data.frame(all_differences)
all_contrasts <- as.data.frame(all_contrasts)
all_contrasts[order(all_contrasts$SE),]  # I'll sort the contrasts by their SE


```


> The first "class" is comparing different varieties within the same field. The corresponding SE is 3.68.
> The second class is comparing variables within the same variety but across diffeerent fields. SE=4.15 
> The third class compares different varities and across different fields. SE=4.38
> This makes intuitive sense because we expect to find the MOST variation in comparisons that are different varieties on completely different fields, like in our third class. It also seems a bit apparent that the three SE's aren't truly that variable, and that may be because we've already noted that variety didn't have that much of an effect so the differences among varities may not be very high.

---

**To explore these differences, we will split the analysis into an analysis of the Fertilizers, and a separate analysis of Varieties**

## Question 2

If we ignore Variety, we can analyze Fertilizer by itself by averaging up to its experimental units.
To do this, we can average the four SubPlots per MainPlot into a single value, resulting in a new smaller dataset.

The following code does this averaging and produces a new data-frame with one observation per MainPlot:
```{r}
mainPlot_data = aggregate(Yield~MainPlot+Fertilizer+Field,oats_data,mean)
str(mainPlot_data)
```

### 2.1 Describe this subset of the experiment in detail, assuming Fields are fixed
This would be our analysis if we were only interested in *these four fields*.
Describe as if this were the only data available

**Design**: 

| Structure | Variable          | Type        | # levels | Experimental Unit |
|-----------|-------------------|-------------|----------|-------------------|
| Treatment | Fertilizer        |    Cat      |   4      |   MainPlot        |
| Design    | Field             |     Cat     |   4      |                   |
| Design    | MainPlot          |     Cat     |   16     |                   |
| Design    | Fert:Field        |     Cat     |   16     |                   |
| Response  | Yield             |    Num      |   16     |                   |


### 2.2 Fit a linear model to the `mainPlot_data`, and present an ANOVA table
```{r}
mainPlot_fixed <- lm(Yield ~ Fertilizer + Field, data=mainPlot_data)

anova(mainPlot_fixed)
  
```


### 2.3 Repeate assuming Blocks are random
This would be our analysis if our fields were a random sample of set of possible fields,
and we wished to make general recommandations for all fields.

**Design**: 

| Structure | Variable          | Type        | # levels | Experimental Unit |
|-----------|-------------------|-------------|----------|-------------------|
| Treatment | Fertilizer        |    Cat      |   4      |    Fert:Field     |
| Design    | Field             |     Cat     |   4      |                   |
| Design    | MainPlot          |     Cat     |   16     |                   |
| Design    | Fert:Field        |     Cat     |   16     |                   |
| Response  | Yield             |    Num      |   16     |                   |


### 2.4 Fit a linear model to the `mainPlot_data`, and present an ANOVA table
```{r}
mainPlot_random <- lmer(Yield ~ Fertilizer + (1|Field), data=mainPlot_data)
#can't include fert:field in the model because it has the same # levels as our response
#have to declare the "randomness" elsewhere
#becuase fert:field is nested in "field"& randomness applies to all nested terms, we'll delcalre field as random

anova(mainPlot_random)

```


### 2.5 Compare the conclusions from these two models about the effects of Fertilizer to the conclusions from the full Split-plot models.
Did treating Block as random affect the conclusions?

> Treating "Field" as random did not affect our conclusion for Fertilizer. The p-value for Fertilizer in every model we've run so far (random & fixed with full data and from this reduced data) is the same at 0.001022. Because the random vs. fixed terms we're declaring as our EU's for Fertilizer in these models are aliased, we shouldn't see a big shift in the effect. 

----

## Question 3

You'll now study the original data for Variety, ignoring Fertilizer.

This design is one we haven't seen before directly. 

We have two hierarchical levels of blocking: 4 Fields, each with 4 MainPlots.

This is called a **Replicated RCBD**. There are effectively 4 replicates of a RCBD, each with 4 different blocks.
The MainPlots are our normal Blocks, and the Fields are now our replications of this experiment.
We say that MainPlots are **nested** in experiment replicates.

### 3.1 Describe this experiment in detail, assuming Fields are fixed
**Note**: We should assume MainPlot is Random. It has to be random given that the whole experiment is a split plot.

**Design**: Replicated RCBD with fixed Reps (Fields) and random blocks (MainPlots)


| Structure | Variable          | Type        | # levels | Experimental Unit            |
|-----------|-------------------|-------------|----------|------------------------------|
| Treatment | Variety           |   Cat       |   4      |    SubPlot                   |
| Design    | Field             |   Cat       |   4      |                              |#fixed rep
|           | MainPlot          |   Cat       |   16     |                              |#random block
|           | Variety:Field     |   Cat       |   16     |                              |#aliased w mainplot
|           | Variety:MainPlot  |   Cat       |   64     |                              |#aliased w subplot
|           | SubPlot           |   Cat       |   64     |                              |
| Response  | Yield             |   Num       |   64     |                              |


### 3.2 Repeat this table, assuming Fields are random

**Design**: Replicated RCBD with Random Reps (Fields) and random blocks (MainPlots)

| Structure | Variable          | Type        | # levels | Experimental Unit            |
|-----------|-------------------|-------------|----------|------------------------------|
| Treatment | Variety           |   Cat       |   4      |  Var:Field                   |
| Design    | Field             |   Cat       |   4      |                              |#random rep
|           | MainPlot          |   Cat       |   16     |                              |#random block
|           | Variety:Field     |   Cat       |   16     |                              |#aliased w mainplot
|           | Variety:MainPlot  |   Cat       |   64     |                              |#aliased w subplot
|           | SubPlot           |   Cat       |   64     |                              |
| Response  | Yield             |   Num       |   64     |                              |


### 3.3 Fit a linear model to the `oats_data` based on the above table (ignoring Fertilizer, and with Field as random).
I've provided the model with fixed blocks.
Which model provides stronger evidence for differences among varieties?

Fixed blocks:
```{r}
subPlot_model_fixed = lmer(Yield ~ Field + (1|MainPlot) + Variety + Variety:Field,oats_data)
anova(subPlot_model_fixed,ddf='K')
```

Random blocks:
```{r}
subPlot_model_random = lmer(Yield ~ (1|Field) + (1|MainPlot) + Variety + (1|Variety:Field), oats_data)

anova(subPlot_model_random,ddf='K')

```

> The Fixed model has slightly lower Variety p-value & slighly higher MSE. Both p-vlaues for both models still not showing a significant effect for the Variety. 

### 3.4 Using both models, form 95% confidence intervals around the estimated difference between Variety_1 and Variety_2
Are the estimates different? The confidence intervals? 
In one sentence each, describe how to interpret each confidence interval

Fixed blocks (Fields) model:
```{r}
fixed_means = emmeans(subPlot_model_fixed,specs = 'Variety',lmer.df = 'K',at = list(Variety = c('V1','V2')))
summary(contrast(fixed_means,method = 'pairwise'),infer = c(T,F))
```

Random blocks (fields) model:
```{r}
random_means = emmeans(subPlot_model_random,specs = 'Variety',lmer.df = 'K',at = list(Variety = c('V1','V2')))
summary(contrast(random_means,method = 'pairwise'),infer = c(T,F))

```

> The estimates of the difference are the same, but the CI's changed. The random model has a wider CI that crosses further over 0 (due to lost df in our critival value). We lost a lot DF by switching from the fixed to the random model. 
> For both CI's, we can interpret them as saying that the mean difference between the Yield of Variety 1 and Variety 2 is -3.63 (not units provided in question), but their CI's both cross zero which means it is possible that there is no difference between the two varieties effects on yield. We cannot reject the null hypothesis.

### 3.5 Compare the ANOVA test on Variety from these models to the Variety test from the SplitPlot models of Question 1.
Which are the same? Which are different?
*Note*: I've provided the code. If you used the model names I suggested above, this should work directly.

Models only with Variety:

Field Fixed:
```{r}
anova(subPlot_model_fixed)
```
Field Random:
```{r}
anova(subPlot_model_random,ddf='K')
```

Models with full data

Field Fixed:
```{r}
anova(fixed_block_model)
```

Field Random:
```{r}
anova(random_block_model,ddf='K')
```

> For both our random models, the Sum Squares & Mean Squares for Variety are different between the two models (higher with reduced data model vs. full data model). The NumDF & DenDF for Variety are the same for both random models. The Variety F-values and P-values are also the same for the two random models. 
> For both our fixed models, the Sum Squares & Mean Squares for Variety are the same, but our F & P-values are different (better in the full data fixed model). Our DenDF was also lower for Variety in the full-data fixed model vs partial data. 

>Some logic behind this: Using a Split Model often improves the precision for comparing the average effects of treatments assigned to subplots (LIKE VARIETY) and, when interactions exist, for detecting those interactions.This arises from the fact that the experimental error for main plots is usually larger than the experimental error used to compare subplot treatments.
