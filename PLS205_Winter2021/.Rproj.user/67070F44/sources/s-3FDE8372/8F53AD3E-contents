---
title: "Midterm Exam grading"
output:
  html_document:
    df_print: paged
---

```{r echo=FALSE,message=FALSE, warning=FALSE}
WITH_CHUNKS = TRUE
library(ggplot2)
library(knitr)
library(emmeans)
library(lme4)
library(lmerTest)
library(car)
library(multcomp)
```

# Instructions

- **Be sure to read every question carefully and answer all parts.**
  - **Clarification questions should be posted to Discussions on Canvas.** Please avoid posting answers, though! No consultation with other students or outside experts is allowed during the exam period (including R programming). You may use any notes / printed material / online resources. You can consult with me or the TAs about R programing.
- **Include all `R` code used to answer each question.** Be sure to include a command so that your results are printed out to the knitted output. This means that if you save your answer in an object like `ans_4.1`, be sure to "print" this object by typing it in a separate line at the end of the `R` code chunk
- Some questions can be answered without any `R` code.
- **Every result should include a discussion.** Full points require a sentence explaining the conclusion.
- **Use http://www.tablesgenerator.com/markdown_tables to easily convert excel tables to markdown** You can create your model tables in Excel, paste them into the website, and then copy the correctly formatted table back into this document.


# Question 1
![Potassium deficient Leaves](potassium_deficient.png)

A researcher is interested in whether almond hulls and shells are useful as organic matter ammendments for almond orchards.
She ran an experiment to test if either ammendment (or their combination) would increase the leaf potassium percentage in almond leaves.
She selected a homogeneous region of an almond orchard, and assigned each row (rows have several trees) to one of four treatments:

1. Control: No ammendment
2. Almond hulls
3. Almond shells
4. Hulls and shells mixed together

Treatments 2-4 were added at a rate equivalent to 150 lb K2O per acre based on the amount of K content of each material. Treatments were applied in the spring.

On a single date in July, 5-8 leaves were sampled from a single tree per row, combined in a sample bag (one bag = one sample), ground to a powder, and submitted for potassium analysis. The assay returns %K per sample.

Data from this experiment are available here:
```{r}
potassium_small = read.csv('Almond_potassium_small.csv')
str(potassium_small)
```

## 1.1 Draw a diagram of the experimental design
Look carefully at the data table above (you can use `View(potassium_small)`, or open in Excel). Your diagram can be very abstracted, but I want to see your interpretation of the overall layout, and levels of sampling. Be as precise as you can given the description above and the information in the data table. Do you see any issues with this experimental layout?

You can either draw using a digital drawing tool, or you can draw on paper, take a picture, and upload your picture. Just save the file in the same directory as this file, and then change the file name below to include the figure:

![](midtermdesign.jpg)

> The image above shows the 16 rows used in the experiment with their treatment application noted at the bottom of each row. Trees in each row are represented by larger rectangles and the small squares within each rectangle represent the leaves that may be sampled (the design doesn't say which tree is sampled when, so they are randomly positioned on this figure). Then the 5-8 leaves collected from a single tree in a single row are put into one sample bag (blue), and ground into a single powder (pink), which is then tested for potassium content.

>I think generally speaking this design is robust. One potential issue is that it doesn't state whether the trees or the leaves are randomly selected to sample within each row. It is possible, for example, that all the Control trees were selected at the northern most edge of the rows while all the Mix trees were sampled from the southern most edge of the row. We might expect some variation in our repsonse due to photosynthetic/other abiotic differences due to spatial heterogenity that might not be accounted for. That said, since our EU is the whole row, rather than the tree, and the rows are successfully interspered spatially, I don't think this prohibits us from moving forward with analysis. 

## 1.2 Create the model table for this experiment
Are there any inconsistencies between your table and the `potassium_small` data.frame?

| Source    | Variable  | Type    | # levels | Experimental Unit |
|-----------|-----------|---------|----------|-------------------|
|Treatment  |Ammendment | Factor  |   4      |Row                |
|Design     |Row        | Factor  |   16     |                   |
|Reponse    |%Potassium | Numeric |   16     |                   |

> Row starts out as an integer, but it should really a factor/categorical so we'll have to switch that below. The other variables look good. We don't need to aggregate anything since there aren't any subsamples, so the rest looks ready to go. The only other difference between my table and the dataframe is that I didn't include "Sample_ID" anywhere, but that's basically a meaningless identifier in this data set since the Rows are already unique IDs and don't represent any combination of treatments/reps.

Fix any inconsistensies in your data here
```{r}
potassium_small$Row = as.factor(potassium_small$Row)
str(potassium_small)

```


## 1.3 Analyze the experiment to answer the question as well as possible: which treatment leads to the highest Potassium level in leaves?
Use alpha = 0.05.

```{r}
#build a model
potass_model <- lm(Potassium ~ Treatment, potassium_small)
summary(potass_model)

#make diagnostic plots
par(mfrow=c(1,2))
plot(potass_model,which=2:3)
#these look pretty good, don't want to transform, so move forward w analysis

#pairwise
means_potass = emmeans(potass_model,spec = 'Treatment')
summary(contrast(means_potass,'pairwise'),infer=T)

#see if these groups are really distinct from one another
cld(means_potass,Letters = letters)

```

> The Mix treatment (combo of both Hulls & Shells) initially seems to have the highest Potassium level, with a mean of 2.323%. Based on my pairwise analysis, it looks like both Hulls & the Mix had higher potassium relative to the control, while Shells and the other pairwise combinations did not have as strong indications of true differences (based on p-values). Running the cld grouping analysis however, showed that we cannot necessarily determine a difference between the Hulls on their own vs. the Mix, so I cannot necessarily say that the Mix treatment definitely results in a higher potassium level than the Hulls treatment does. I can say with confidence that both those two treatments resulted in higher Potassium than the Control, but can't necessarily distinguish a single "highest" outcome or best treatment option. All that said, I'll go with the Mix treatment for the analysis below since it has the highest mean %K. 


## 1.4 Construct confidence intervals for the difference between the treatment that had the highest mean and the control.
Use alpha = 0.05.
This is a follow-up test to your test above, so you should construct confidence intervals that account for the total number of comparisons involved.

```{r}
#trt.vs.control
effects = contrast(means_potass,'trt.vs.ctrl',ref=1)
#confidence interval
summary(effects,level = 0.95,infer = c(T,F))
#find the cv  using upper.CL
#1.91+cv*0.371 = 2.91
#1=0.371*cv
#cv=2.695

#try alternate approach with Bonferroni correction on subsetted data
subset_potass <- subset(potassium_small,Treatment=='Control' | Treatment == 'Mix')
subset_model<-lm(Potassium~Treatment, subset_potass)
means_subset = emmeans(subset_model,spec = 'Treatment')
#bonferroni correction
temp<-contrast(means_subset,'pairwise')
confint(temp, adjust = "bonferroni")
#find the cv using upper.CL
#-1.91+cv*0.185 = -1.46
#0.45=0.185*cv
#cv=2.432


```

> It's not super clear whether this question wants us to create a CI of the highest mean vs control contrast under the assumption that data is treated separately (subsetted) or as a whole group. Because of this, I did both to answer either question & in both cases the intervals account for the total number of comparisons involved.

>Using the classic treatment vs. control Dunnett method, I got the CI [0.9066, 2.91].

>To create a Bonferroni corrected confidence interval of subsetted data, I first selected the Mix & Control data as a subset of the total data to compare just the two of them. Because that subsetted comparison isn't exactly pairwise, nor trt.vs.control, I then applied the Bonferroni Correction to get the new "corrected" CI of [-2.36, -1.46]. 


## 1.5 Explain the following from your results above:

What was the degrees of freedom, and why did it have this value?
What was the estimated SED of this comparison? Given this, what is s^2?
What is the critical value that was used to create the confidence intervals?

> The degrees of freedom in the contrast function is always N-k (N observations - k treatments). For the original Dunnett test, the number of observations is 16 - 4 treatments to give us Df=12. For the Bonferroni test, the number of subsetted observations is 8 - 2 treatments = 6.

>The estimated SED for the original function is 0.371. SED = sqrt((s^2)/n/2) so if we solve for s^2 we get (SED^2)*(n/2). n is the number of observations included in the comparison, which is 8. For the original function that gives us s^2 = 0.55. For the Bonferroni comparison, our SED is 0.185. n stays the same so our s^2 = 0.14.

>The critical value for the original comparison was 2.695 and the cv for the Bonferroni comparison was 2.432. Back-calculations of cv's included in coded #'s above.

# Question 2
Say instead of collecting leaves from only one tree, the research had actually collected 5-8 leaves from each of 3 trees per row.
Again, leaves from each single tree were bagged, ground, and sent for assays. Otherwise, the experiment was identical to the description above.

Data from this version of the experiment are available here:
```{r}
potassium_large = read.csv('Almond_potassium_large.csv')
str(potassium_large)
```

## 2.1 Create a model table for this experiment.
Again, describe any inconsistencies and this time be sure to correct the data.frame so it is completely consistent with your table.

| Source    | Variable  | Type    | # levels | Experimental Unit |
|-----------|-----------|---------|----------|-------------------|
|Treatment  |Ammendment |Factor   |   4      |Row                |
|Design     |Row        |Factor   |   16     |                   |
|Design     |Tree       |Factor   |   48     |                   |
|Response   |Potassium% |Numeric  |   48     |                   |

> We'll have to edit the data so that both row & tree are treated as categorical/factors. I might also add a new column that creates an interaction term between Row & Tree so I can more clearly assess trees as subsamples of our EU.

Fix any inconsistensies in your data here
```{r}
#make row a factor again
potassium_large$Row = as.factor(potassium_large$Row)

#make tree a factor
potassium_large$Tree = as.factor(potassium_large$Tree)

#new column that connects tree & row as a unique ID
potassium_large$SubSample <- interaction(potassium_large$Row,potassium_large$Tree,drop = TRUE)

str(potassium_large)

```

## 2.2 Repeat the analysis from question 1.3 for this new larger experiment.
Compare your answers.
Why are the estimates of the treatment means different?
Why are the standard errors different? (be very specific. For full credit, you should give the equation for the standard error of a mean, and describe which parts of this equation differ between this experiment and the first experiment, and why)

```{r}
#build a model
potass_lg_model <- lmer(Potassium ~ Treatment + (1|Row), potassium_large)

#make diagnostic plots to test assumptions
#aggregate the means
potass_lg_means = aggregate(Potassium ~ Treatment + Row, potassium_large,FUN = mean)
potass_lg_means$deviation = ranef(potass_lg_model)$Row[potass_lg_means$Row,1] 
potass_lg_means$fitted = predict(potass_lg_model,newdata = potass_lg_means,re.form = ~0)
# Step 2: Make QQplot and `Scale-Location` plot:
op = par(mfrow=c(1,2))  # two plots side-by-side
qqPlot(potass_lg_means$deviation,main = 'Plot (EU) Normal Q-Q',pch=19)  # new qqplot function
scatter.smooth(potass_lg_means$fitted,sqrt(abs(potass_lg_means$deviation)),span = 1,main = 'Scale-Location',ylab = expression(sqrt(abs(' deviations '))),xlab = 'Fitted values')


#treatment vs control for our new model
means_potass_lg = emmeans(potass_lg_model,specs = 'Treatment', lmer.df = 'k')  # This preps an emmeans analysis, grouping by treatment
  # if our model is from lmer(), we include lmer.df = 'k'. 
summary(means_potass_lg,level = 0.95,infer = c(T,F))  
differences_vs_control = contrast(means_potass_lg,method = 'trt.vs.ctrl',ref = 1)  # ref = 6, because the control is the 6th Strain
#if you don't know which is the control, do levels(model)
summary(differences_vs_control,level = 0.95,infer = c(T,F))
summary(differences_vs_control,level = 0.95,infer = c(F,T))

#to compare SE & means, while also including cld
means_potass2 = emmeans(potass_lg_model,spec = 'Treatment')
cld(means_potass2,Letters = letters)

#compare means_potass & means_potass2

```

>Comparison: Our values defintely changed! The treatment means for the Control group and the Mix group both increased, while the means for the Shells and the Hulls decreased. The standard error of the mens decreased. The CI's became narrower in this experiment and we can make clearer pairwise distinctions as seen by the cld groups. 

> The estimates of the treatment means are different because they are based on more values! We have more subsamples per sample which is changing/reducing the within-sample variability. Also, there's just more and different data included, so the means are different because they reflect new data, even if not every subsample increases precision. 

>The standard error of the mean is cacluated as the square root of the estimated variance divided by the number of samples (sqrt((s^2)/n)). n represents the number of EU samples, so that is staying the same between our two tests, since the number of ROWS does not change, only the number of TREES which is our subsample. 

>s^2 is the part of that equation DOES change. s^2 represents the variance among the row-level observations relative to the overall sample mean. In this new experiment, we aggregate each sub-sample into a single observation per row before estimating our among-observation s^2. Previously, we just had one single sample per row, but this time that per-row observation is already a mean of 3 subsamples taken at different trees within the row. If that aggregated row-level observation is more precise (& theoretically increasing replicates increases precision), then we may find less variability among observations which trickles up all the way to the SE. In this experiment we got a lower SEM than the previous experiment, so we can infer that the subsamples did increase the precision of each sample observation, which decreased variation among sample estimates, which then decreased the SE. 

## 2.3 Given the results above, would you recommend that the researcher uses more or fewer trees-per-row if she were to repeat this experiment?
Explain your reasoning. Your answer doesn't have to be mathematically justfied, but your logic should be clear.

> I would recommend that the researcher use more trees-per-row as long as doing so didn't add very high financial costs to the experiment. Because trees operate as subsamples in our design, adding a higher number of them will hopefully increase the precision (theoretically, but really depends on the study system) of our within treatment means and variances. Because the trees are just subsamples, they are not increasing the overall # of EU, and thus not changing our degrees of freedom, which might have opposite effects on the strength of our analyses. As a result, we would expect that our SE would decrease as described in the section above, which is great! This would happen because our pooled within treatment variance (s^2) would decrease while our overall N stays the same. Then, with our decreased SE, we could also calculate narrower Confidence Intervals around the means since they rely on SE and our Df that's going into the critical value remains the same. 

>All this said, it's possible that there are diminishing returns with increasing subsamples. For example, if our current estimates are already very close to the true population estimates, then there isn't a lot of value in adding additional subsamples or additional EU samples for that matter. It's also possible that adding new subsamples could introduce new variation rather than increasing precision, because the world is a wild and unpredictable place. If that were the case, the resulting SE & CI's could have the opposite effect and be higher & wider as a result of added subsamples. 

# Question 3
![Corn field](Corn_density.jpg)
A researcher wanted to find the optimum seeding density for corn in Northern California.
He chose 4 planting densities of 2, 4, 6 and 8 plants / m^2, and randomly assigned each to several plots in a field.
He measured yield at the end of the season in each plot.

To decide on an optimal yield, he started by running a trend analysis as described in class.

## 3.1 Based on the plots below, describe the implications of choosing the degree=1 or degree=3 models relative to the degree=2 model
Use words: "Bias" and "Precision"
Focus specifically on predictions around Density = 2.5 and Density = 6.5.

```{r, fig.width = 4}
include_graphics('Yield_trends.png')
```

> Overall, the Degree-1 model looks pretty bad. We can see it's very biased since the line does not actually hit any of the treatment means. It is also not very precise since it has a pretty wide CI around the line. Model 2 seems to be the most precise with relatively low bias. Model 3 is the least biased but also has some areas of poor precision, like from 0-2. There seems to be a tradeoff between precision and bias between 2 and 3.

>At density 2.5, model 2 seems to perform the best. It has the highest precision, and maybe reasonably low bias relative to the other two. Hard to judge exactly the value there, but it seems like models 2 & 3 predict a pretty similar yield at 2.5, so they may have the same level of bias. Model 1 has very low precision at 2.5 with its large CI. 

>At density 6.5, it seems like maybe model 3 performs better, in that it's probably the closest to the true mean (least biased). 1, 2, and 3 all have low precision at density 6.5.


## 3.2 Below is the ANOVA table for his full model.

| Source    | Df | Sum Sq  | Mean Sq | F-value | Pr(>F)   |
|-----------|----|---------|---------|---------|----------|
| Density   | 1  | 11.8141 | 11.8141 | 52.335  | 1.62E-08 |
| Density^2 | 1  | 15.3497 | 15.3497 | 67.9973 | 8.21E-10 |
| Density^3 | 1  | 1.4871  | 1.4871  | 6.5877  | 0.01457  |
| Residuals | 36 | 8.1266  | 0.2257  |         |          |

How many total plots were there? 
Based on this table, what model would you choose and why?

> There are 40 plots. Dfe = Total # observation - # groups being compared. So 36 = #plots - 4 (including the null model), so # plots = 40. 

>Based on the table, I would choose the degree-2 model. Degree-2 has a much lower p-value than Degree-3 which is reflective of the decreased precision we saw in the plots above that happens when you increase complexity. Degree-2 also has a higher p-value than Degree-1, so we can infer that the added complexity of model 2 is adding precision that's not at too high a cost for bias. 

## 3.3 Say he decided to fit a Degree-2 model instead. Fill in the ANOVA table for this model

| Source    | Df | Sum Sq  | Mean Sq | F-value | Pr(>F)   |
|-----------|----|---------|---------|---------|----------|
| Density   | 1  | 11.8141 | 11.8141 | 45.468  | 6.25E-08 |
| Density^2 | 1  | 15.3497 | 15.3497 | 59.083  | 3.52E-09 |
| Residuals | 37 | 9.6137  | 0.2598  |         |          |

Use what you know about how the ANOVA works to fill out this table.
*Hint: the total Sums of Squares and Degrees of Freedom don't change when you change the model, only their distribution among the model terms changes.*
  Did the MSE increase or decrease for the reduced model (Degree-2) relative to the full model (Degree-3)?
  What implication does this have for his prediction intervals?

>No code block here, but I did use code to calculate the p-value: pf(59.083, DfT=1, DfE=37, lower.tail=F) 

>The MSE increases for the reduced model relative to the full model. Because MSE is essentially equal to the variability within treatments (s^2), we infer that this variability increased and confidence intervals widened when we reduced the model degree. His prediction intervals will be less precise with the reduced model as opposed to using the full model. But also, take it with a grain of salt, because the difference between the two MSE values was roughly 0.03, so that may not result in a meaningful enough change in precision to really cause concern.
